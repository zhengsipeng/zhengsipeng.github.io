---
permalink: /
title: "Sipeng Zheng"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about.html
---

I'm Sipeng Zheng, a researcher affiliated with the Multimodal Interaction Research Group at the [Beijing Academy of Artificial Intelligence(BAAI)](https://www.baai.ac.cn), , where I collaborate with [Prof. Zongqing Lu](https://z0ngqing.github.io). 
I obtained my PhD and bachelor degree from [Renmin University of China (RUC)](https://en.ruc.edu.cn), under the guidance of [Prof. Qin Jin](https://www.jin-qin.com). 
My primary research focus revolves around human behavior understanding, vision-and-language learning, and embodied artificial intelligence.
My CV can be found in [CV](https://zhengsipeng.github.io/cv/) or [downloaded CV](http://zhengsipeng.github.io/files/cv_zsp_eng.pdf).


## Research Interest
* Computer vision and human behavior understanding
* Multimodal understanding with large language models
* Open-world embodied agent learning


## Work Experience
* Jul. 2023 - Current: Research Scientist
  * Beijing Academy of Artificial Intelligence, Beijing, China
  * Duties included: large multimodal model, multi-agent learning


* Apr. 2022 - Oct. 2022: Research Intern
  * Microsoft Research Asia, Beijing, China
  * Duties included: temporal sentence grounding for long-term videos.

* Nov. 2021 - Apr. 2022: Research Intern
  * Beijing Academy of Artificial Intelligence, Beijing, China
  * Duties included: multi-lingual language-vision-audio pre-training.


## Education
* B.Eng. in Computer Science and Engineering, Renmin University of China, China, 2023
* Ph.D. in Computer Science and Engineering, Renmin University of China, China, 2023


## Publications
<table border="0" style="border:none; width: 100%;">
<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/steve_eye_23.png" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
        <p><strong>Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</strong><br>
        <b>Sipeng Zheng</b>, Jiazheng Liu, Yicheng Feng, Zongqing Lu<br>
        arxiv
        </p>
        <p> 
          [<a target="_blank" href="https://arxiv.org/abs/2310.13255">pdf</a>] 
          [<a target="_blank" href="https://github.com/BAAI-Agents/Steve-Eye">code</a>] 
          [<a target="_blank" href="https://sites.google.com/view/steve-eye">page</a>]
        </p>
  </td>
</tr>

<tr style="border: none;">
  <td> <img src="./images/pubs/llama_rider_23.png" style="height: 100px; width: 500px;"/></td>
  <td> 
      <p><strong>LLaMA Rider: Spurring Large Language Models to Explore the Open World</strong><br>
        Yicheng Feng, Yuxuan Wang, Jiazheng Liu, <b>Sipeng Zheng</b>, Zongqing Lu<br>
        arxiv
      </p>
      <p> 
          [<a target="_blank" href="https://arxiv.org/abs/2310.08922">pdf</a>] 
          [<a target="_blank" href="https://github.com/PKU-RL/LLaMA-Rider">code</a>] 
      </p>
  </td>
</tr>

<tr style="border: none;">
  <td> <img src="./images/pubs/mm23_pov.png" style="height: 100px; width: 500px;"/></td>
  <td> 
      <p><strong>POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World</strong><br>
        Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
        ACM MM, 2023
      </p>
      <p> 
          [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3581783.3612484">pdf</a>] 
          [<a target="_blank" href="https://github.com/xuboshen/pov_acmmm2023">code</a>] 
      </p>
  </td>
</tr>

</table>


* Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds (2023). 
**Sipeng Zheng**, Jiazheng Liu, Yicheng Feng, Zongqing Lu. 

[[pdf]](https://arxiv.org/abs/2310.13255)
[[page]](https://sites.google.com/view/steve-eye) 
[[code]](https://github.com/BAAI-Agents/Steve-Eye)


* LLaMA Rider: Spurring Large Language Models to Explore the Open World (2023). 
Yicheng Feng, Yuxuan Wang, Jiazheng Liu, **Sipeng Zheng**, Zongqing Lu. 
[[pdf]](https://arxiv.org/abs/2310.08922)
[[code]](https://github.com/PKU-RL/LLaMA-Rider)


* POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World (ACM MM 2023).  
Boshen Xu, **Sipeng Zheng**, Qin Jin.
[[pdf]](https://dl.acm.org/doi/10.1145/3581783.3612484)
[[page]](https://xuboshen.github.io/POV/) 
[[[code]]](https://github.com/xuboshen/pov_acmmm2023)


* No-frills Temporal Video Grounding: Multi-Scale Neighboring Attention and Zoom-in Boundary Detection (2023). 
Qi Zhang, **Sipeng Zheng**, Qin Jin.
[[pdf]](https://arxiv.org/abs/2307.10567)


* Open-Category Human-Object Interaction Pre-Training via Language Modeling Framework (CVPR 2023). 
**Sipeng Zheng**, Boshen Xu, Qin Jin.
[[pdf]](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Open-Category_Human-Object_Interaction_Pre-Training_via_Language_Modeling_Framework_CVPR_2023_paper.pdf)


* Accommodating audio modality in CLIP for multimodal processing (AAAI 2023). 
Ludan Ruan, Anwen Hu, Yuqing Song, Lliang Zhang, **Sipeng Zheng**, Qin Jin. 
[[pdf]](https://ojs.aaai.org/index.php/AAAI/article/view/26153/25925)


* Anchor-Based Detection for Natural Language Localization in Ego-Centric Videos (IEEC 2023). 
**Sipeng Zheng**, Bei Liu, Jianlong Fu, Wen-Huang Cheng. 
[[pdf]](https://ieeexplore.ieee.org/abstract/document/10043460)
[[code]](https://github.com/QiQAng/AwareNet)


* Few-shot Action Recognition with Hierarchical Matching and Contrastive Learning (ECCV 2022). 
**Sipeng Zheng**, Shizhe Chen, Qin Jin.
[[pdf]](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640293.pdf)
[[code]](https://github.com/zhengsipeng/HCL-FSAR)


* VRDFormer: End-to-end video visual relation detection with transformer (CVPR 2022 Oral).  
**Sipeng Zheng**, Shizhe Chen, Qin Jin.
[[pdf]](https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_VRDFormer_End-to-End_Video_Visual_Relation_Detection_With_Transformers_CVPR_2022_paper.pdf)
[[code]](https://github.com/zhengsipeng/VRDFormer_VRD)


* Exploring anchor-based detection for ego4d natural language query (CVPR 2022 workshop). 
**Sipeng Zheng**, Qi Zhang, Bei Liu, Qin Jin, Jianlong Fu. 
[[pdf]](https://arxiv.org/abs/2208.05375)
[[code]](https://github.com/QiQAng/AwareNet)


* Skeleton-based interactive graph network for human object interaction detection (ICME 2020). 
**Sipeng Zheng**, Shizhe Chen, Qin Jin.
[[pdf]](https://ieeexplore.ieee.org/document/9102755)
[[code]](https://github.com/zhengsipeng/SIGN)


* Visual relation detection with multi-level attention (ACM MM 2019). 
**Sipeng Zheng**, Shizhe Chen, Qin Jin.
[[pdf]](https://dl.acm.org/doi/10.1145/3343031.3350962)


* Relation understanding in videos (ACM MM 2019 grand challenge). 
**Sipeng Zheng**, Xiangyu Chen, Shizhe Chen, and Qin Jin.
[[pdf]](https://dl.acm.org/doi/10.1145/3343031.3356080)

  
## Awards
* National Scholarship for Ph.D Students.
* 2022 Ranked 3th in CVPR 2022 Ego4D Natural Language Query Challenge.
* 2021 Ranked 3th in NIST TRECVID 2021 Ad-hoc Video Search (AVS) Challenge. (20+ teams)
* 2021 Ranked 4th in CVPR 2021 HOMAGE Scene-graph Generation Challenge.
* 2020 Ranked 2th in ACM MM 2020 Video Relationship Understanding Grand Challenge.
* 2019 Ranked 2nd in ACM MM 2019 Video Relationship Understanding Grand Challenge.
* Best Method Prize in ACM MM 2019 Grand Challenge.
* 2019 First Class Scholarship for Ph.D Students from 2018 to 2021.
* 2015 First Prize in National University Mathematical Modeling Competition of Beijing Area.

## Services
* Conference Reviewer for CVPR, ICCV, ECCV, NeurIPS, AAAI, ACM MM, ICME.
* Journal Reviewer for IJCV, TCSVT, TMM, JATS.
