---
permalink: /
title: "Sipeng Zheng"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about.html
---

<b>Welcome to My Homepage!</b>
I'm a general partner of <b>BeingBeyond</b>, a startup dedicated to advancing large foundation models for humanoid robots, where I collaborate closely [Prof. Zongqing Lu](https://z0ngqing.github.io). 
Prior to this, I was a researcher at the [Beijing Academy of Artificial Intelligence(BAAI)](https://www.baai.ac.cn).
I obtained my PhD and bachelor's degree from [Renmin University of China (RUC)](https://en.ruc.edu.cn), under the guidance of [Prof. Qin Jin](https://www.jin-qin.com). 
My research primarily focuses on human behavior understanding, vision-and-language learning, and the development of open-world embodied agents.
Currently I'm working towards an general-purpose humanoid robot.
For more details, please refer to my [CV](http://zhengsipeng.github.io/cv_zsp_en.pdf).

<b>Join Us!</b>

We are actively recruiting full-time researchers and interns to join our team. If you’re passionate about embodied AI, feel free to [reach out](zhengsipeng27@gmail.com).


## Research Interest
* Large language models and large multimodal models
* Open-world embodied agent learning
* Human behavior and motion understanding
* Robot learning


## Work Experience
* May. 2025 - Current: Research Scientist
  * BeingBeyond, Beijing, China
  * Duties included: large multimodal model, robot learning, human motion understanding

* Jul. 2023 - May. 2025: Research Scientist
  * Beijing Academy of Artificial Intelligence, Beijing, China
  * Duties included: large multimodal model, multi-agent learning


* Apr. 2022 - Oct. 2022: Research Intern
  * Microsoft Research Asia, Beijing, China
  * Duties included: temporal sentence grounding for long-term videos.

* Nov. 2021 - Apr. 2022: Research Intern
  * Beijing Academy of Artificial Intelligence, Beijing, China
  * Duties included: multi-lingual language-vision-audio pre-training.


## Education
* B.Eng. in Computer Science and Engineering, Renmin University of China, China, 2023
* Ph.D. in Computer Science and Engineering, Renmin University of China, China, 2023



## Publications

<b>* denotes equal contribution</b>

<!-- 分页容器 -->
<div class="pagination-container">
  <!-- 这里将由JavaScript动态生成分页内容 -->
   <div class="paper-item">
    <table style="border: none; width: 100%;">
      <tr style="border: none;">
        <td style="border: none; width: 500px;"> 
          <img src="./images/10-mm23_pov.png" width="500px" height="100px"/>
        </td>
        <td style="border: none;"> 
          <p style="font-size: 15px">
            <b style="font-size: 18px">POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World</b><br>
            Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
            ACM MM, 2023<br>
            [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3581783.3612484">pdf</a>] 
            [<a target="_blank" href="https://github.com/xuboshen/pov_acmmm2023">code</a>]
            [<a target="_blank" href="https://xuboshen.github.io/POV/">page</a>] 
          </p>
        </td>
      </tr>
    </table>
  </div>

   <div class="paper-item">
    <table style="border: none; width: 100%;">
      <tr style="border: none;">
        <td style="border: none; width: 500px;"> 
          <img src="./images/10-mm23_pov.png" width="500px" height="100px"/>
        </td>
        <td style="border: none;"> 
          <p style="font-size: 15px">
            <b style="font-size: 18px">POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World</b><br>
            Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
            ACM MM, 2023<br>
            [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3581783.3612484">pdf</a>] 
            [<a target="_blank" href="https://github.com/xuboshen/pov_acmmm2023">code</a>]
            [<a target="_blank" href="https://xuboshen.github.io/POV/">page</a>] 
          </p>
        </td>
      </tr>
    </table>
  </div>

   <div class="paper-item">
    <table style="border: none; width: 100%;">
      <tr style="border: none;">
        <td style="border: none; width: 500px;"> 
          <img src="./images/10-mm23_pov.png" width="500px" height="100px"/>
        </td>
        <td style="border: none;"> 
          <p style="font-size: 15px">
            <b style="font-size: 18px">POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World</b><br>
            Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
            ACM MM, 2023<br>
            [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3581783.3612484">pdf</a>] 
            [<a target="_blank" href="https://github.com/xuboshen/pov_acmmm2023">code</a>]
            [<a target="_blank" href="https://xuboshen.github.io/POV/">page</a>] 
          </p>
        </td>
      </tr>
    </table>
  </div>

   <div class="paper-item">
    <table style="border: none; width: 100%;">
      <tr style="border: none;">
        <td style="border: none; width: 500px;"> 
          <img src="./images/10-mm23_pov.png" width="500px" height="100px"/>
        </td>
        <td style="border: none;"> 
          <p style="font-size: 15px">
            <b style="font-size: 18px">POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World</b><br>
            Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
            ACM MM, 2023<br>
            [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3581783.3612484">pdf</a>] 
            [<a target="_blank" href="https://github.com/xuboshen/pov_acmmm2023">code</a>]
            [<a target="_blank" href="https://xuboshen.github.io/POV/">page</a>] 
          </p>
        </td>
      </tr>
    </table>
  </div>

   <div class="paper-item">
    <table style="border: none; width: 100%;">
      <tr style="border: none;">
        <td style="border: none; width: 500px;"> 
          <img src="./images/10-mm23_pov.png" width="500px" height="100px"/>
        </td>
        <td style="border: none;"> 
          <p style="font-size: 15px">
            <b style="font-size: 18px">POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World</b><br>
            Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
            ACM MM, 2023<br>
            [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3581783.3612484">pdf</a>] 
            [<a target="_blank" href="https://github.com/xuboshen/pov_acmmm2023">code</a>]
            [<a target="_blank" href="https://xuboshen.github.io/POV/">page</a>] 
          </p>
        </td>
      </tr>
    </table>
  </div>

   <div class="paper-item">
    <table style="border: none; width: 100%;">
      <tr style="border: none;">
        <td style="border: none; width: 500px;"> 
          <img src="./images/10-mm23_pov.png" width="500px" height="100px"/>
        </td>
        <td style="border: none;"> 
          <p style="font-size: 15px">
            <b style="font-size: 18px">POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World</b><br>
            Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
            ACM MM, 2023<br>
            [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3581783.3612484">pdf</a>] 
            [<a target="_blank" href="https://github.com/xuboshen/pov_acmmm2023">code</a>]
            [<a target="_blank" href="https://xuboshen.github.io/POV/">page</a>] 
          </p>
        </td>
      </tr>
    </table>
  </div>

   <div class="paper-item">
    <table style="border: none; width: 100%;">
      <tr style="border: none;">
        <td style="border: none; width: 500px;"> 
          <img src="./images/10-mm23_pov.png" width="500px" height="100px"/>
        </td>
        <td style="border: none;"> 
          <p style="font-size: 15px">
            <b style="font-size: 18px">POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World</b><br>
            Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
            ACM MM, 2023<br>
            [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3581783.3612484">pdf</a>] 
            [<a target="_blank" href="https://github.com/xuboshen/pov_acmmm2023">code</a>]
            [<a target="_blank" href="https://xuboshen.github.io/POV/">page</a>] 
          </p>
        </td>
      </tr>
    </table>
  </div>

   <div class="paper-item">
    <table style="border: none; width: 100%;">
      <tr style="border: none;">
        <td style="border: none; width: 500px;"> 
          <img src="./images/10-mm23_pov.png" width="500px" height="100px"/>
        </td>
        <td style="border: none;"> 
          <p style="font-size: 15px">
            <b style="font-size: 18px">POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World</b><br>
            Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
            ACM MM, 2023<br>
            [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3581783.3612484">pdf</a>] 
            [<a target="_blank" href="https://github.com/xuboshen/pov_acmmm2023">code</a>]
            [<a target="_blank" href="https://xuboshen.github.io/POV/">page</a>] 
          </p>
        </td>
      </tr>
    </table>
  </div>

   <div class="paper-item">
    <table style="border: none; width: 100%;">
      <tr style="border: none;">
        <td style="border: none; width: 500px;"> 
          <img src="./images/10-mm23_pov.png" width="500px" height="100px"/>
        </td>
        <td style="border: none;"> 
          <p style="font-size: 15px">
            <b style="font-size: 18px">POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World</b><br>
            Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
            ACM MM, 2023<br>
            [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3581783.3612484">pdf</a>] 
            [<a target="_blank" href="https://github.com/xuboshen/pov_acmmm2023">code</a>]
            [<a target="_blank" href="https://xuboshen.github.io/POV/">page</a>] 
          </p>
        </td>
      </tr>
    </table>
  </div>

   <div class="paper-item">
    <table style="border: none; width: 100%;">
      <tr style="border: none;">
        <td style="border: none; width: 500px;"> 
          <img src="./images/10-mm23_pov.png" width="500px" height="100px"/>
        </td>
        <td style="border: none;"> 
          <p style="font-size: 15px">
            <b style="font-size: 18px">POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World</b><br>
            Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
            ACM MM, 2023<br>
            [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3581783.3612484">pdf</a>] 
            [<a target="_blank" href="https://github.com/xuboshen/pov_acmmm2023">code</a>]
            [<a target="_blank" href="https://xuboshen.github.io/POV/">page</a>] 
          </p>
        </td>
      </tr>
    </table>
  </div>

   <div class="paper-item">
    <table style="border: none; width: 100%;">
      <tr style="border: none;">
        <td style="border: none; width: 500px;"> 
          <img src="./images/10-mm23_pov.png" width="500px" height="100px"/>
        </td>
        <td style="border: none;"> 
          <p style="font-size: 15px">
            <b style="font-size: 18px">POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World</b><br>
            Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
            ACM MM, 2023<br>
            [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3581783.3612484">pdf</a>] 
            [<a target="_blank" href="https://github.com/xuboshen/pov_acmmm2023">code</a>]
            [<a target="_blank" href="https://xuboshen.github.io/POV/">page</a>] 
          </p>
        </td>
      </tr>
    </table>
  </div>
</div>

<style>
.pagination {
  position: relative;
  margin-top: 20px;
}
.page-labels {
  text-align: center;
  margin: 20px 0;
}
.page-labels label {
  display: inline-block;
  padding: 5px 10px;
  margin: 0 5px;
  border: 1px solid #ddd;
  cursor: pointer;
  border-radius: 3px;
  transition: all 0.3s ease;
}
.page-labels label:hover {
  background-color: #f0f0f0;
}
input[type="radio"] {
  display: none;
}
.page {
  display: none;
  animation: fadeIn 0.5s ease;
}
.page table {
  border: none;
  width: 100%;
}
@keyframes fadeIn {
  from { opacity: 0; }
  to { opacity: 1; }
}
</style>

<script>
document.addEventListener('DOMContentLoaded', function() {
  // 获取所有论文项（假设每篇论文都有.paper-item类）
  const paperItems = document.querySelectorAll('.paper-item');
  const totalPapers = paperItems.length;
  const papersPerPage = 10; // 每页显示10篇论文
  
  // 计算总页数
  const totalPages = Math.ceil(totalPapers / papersPerPage);
  
  // 获取分页容器
  const paginationContainer = document.querySelector('.pagination-container');
  
  // 如果论文数量不足一页，直接显示所有论文
  if (totalPages <= 1) {
    const table = document.createElement('table');
    table.style.border = 'none';
    table.style.width = '100%';
    paperItems.forEach(item => {
      table.appendChild(item.cloneNode(true));
    });
    paginationContainer.appendChild(table);
    return;
  }
  
  // 创建分页结构
  const paginationDiv = document.createElement('div');
  paginationDiv.className = 'pagination';
  
  // 创建单选框
  for (let i = 1; i <= totalPages; i++) {
    const radio = document.createElement('input');
    radio.type = 'radio';
    radio.name = 'pages';
    radio.id = `page${i}`;
    if (i === 1) radio.checked = true;
    paginationDiv.appendChild(radio);
  }
  
  // 创建页码标签
  const labelsDiv = document.createElement('div');
  labelsDiv.className = 'page-labels';
  for (let i = 1; i <= totalPages; i++) {
    const label = document.createElement('label');
    label.htmlFor = `page${i}`;
    label.textContent = i;
    labelsDiv.appendChild(label);
  }
  paginationDiv.appendChild(labelsDiv);
  
  // 创建页面内容
  const contentDiv = document.createElement('div');
  contentDiv.className = 'page-content';
  
  // 将论文分配到各页
  for (let page = 1; page <= totalPages; page++) {
    const pageDiv = document.createElement('div');
    pageDiv.className = 'page';
    
    const table = document.createElement('table');
    table.setAttribute('border', '0');
    table.style.border = 'none';
    table.style.width = '100%';
    
    const startIndex = (page - 1) * papersPerPage;
    const endIndex = Math.min(startIndex + papersPerPage, totalPapers);
    
    for (let i = startIndex; i < endIndex; i++) {
      table.appendChild(paperItems[i].cloneNode(true));
    }
    
    pageDiv.appendChild(table);
    contentDiv.appendChild(pageDiv);
  }
  
  paginationDiv.appendChild(contentDiv);
  paginationContainer.appendChild(paginationDiv);
  
  // 动态生成CSS
  const style = document.createElement('style');
  let cssText = '';
  
  // 为每页创建选择器
  for (let i = 1; i <= totalPages; i++) {
    cssText += `
      #page${i}:checked ~ .page-content .page:nth-child(${i}) {
        display: block;
      }
      #page${i}:checked ~ .page-labels label[for="page${i}"] {
        background-color: #0066cc;
        color: white;
        border-color: #0066cc;
      }
    `;
  }
  
  style.textContent = cssText;
  document.head.appendChild(style);
});
</script>

  
## Awards
* 2025 Ranked 1st in GemBench Challenge at CVPR 2025 Workshop GRAIL.
* 2022 Ranked 3th in CVPR 2022 Ego4D Natural Language Query Challenge.
* 2021 Ranked 3th in NIST TRECVID 2021 Ad-hoc Video Search (AVS) Challenge.
* 2021 Ranked 2nd in CVPR 2021 HOMAGE Scene-graph Generation Challenge.
* 2020 Ranked 2nd in ACM MM 2020 Video Relationship Understanding Grand Challenge.
* 2019 Ranked 2nd in ACM MM 2019 Video Relationship Understanding Grand Challenge.
* 2022 National Scholarship for Ph.D Students.
* Best Method Prize in ACM MM 2019 Grand Challenge.
* 2019 First Class Scholarship for Ph.D Students from 2018 to 2021.
* 2015 First Prize in National University Mathematical Modeling Competition of Beijing Area.

## Services
* Conference Reviewer for CVPR, ICCV, ECCV, ACCV, NeurIPS, AAAI, ACM MM.
* Journal Reviewer for IJCV, TCSVT, TMM, JATS.
