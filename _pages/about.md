---
permalink: /
title: "Sipeng Zheng"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about.html
---

I'm Sipeng Zheng, currently working as a researcher with the Multimodal Interaction Research Group at the [Beijing Academy of Artificial Intelligence(BAAI)](https://www.baai.ac.cn), where I collaborate with [Prof. Zongqing Lu](https://z0ngqing.github.io). 
I obtained my PhD and bachelor's degree from [Renmin University of China (RUC)](https://en.ruc.edu.cn), under the guidance of [Prof. Qin Jin](https://www.jin-qin.com). 
My research primarily focuses on human behavior understanding, vision-and-language learning, and the development of open-world embodied agents.
Currently I'm working towards an intelligent humanoid robot.
For more details, please refer to my [CV](http://zhengsipeng.github.io/cv_zsp_en.pdf).


## Research Interest
* Human behavior understanding
* Large language models and large multimodal models
* Open-world embodied agent learning
* Human motion understanding


## Work Experience
* Jul. 2023 - Current: Research Scientist
  * Beijing Academy of Artificial Intelligence, Beijing, China
  * Duties included: large multimodal model, multi-agent learning


* Apr. 2022 - Oct. 2022: Research Intern
  * Microsoft Research Asia, Beijing, China
  * Duties included: temporal sentence grounding for long-term videos.

* Nov. 2021 - Apr. 2022: Research Intern
  * Beijing Academy of Artificial Intelligence, Beijing, China
  * Duties included: multi-lingual language-vision-audio pre-training.


## Education
* B.Eng. in Computer Science and Engineering, Renmin University of China, China, 2023
* Ph.D. in Computer Science and Engineering, Renmin University of China, China, 2023


## Publications
<table border="0" style="border:none; width: 100%;">

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/videoorin_24.jpg" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
        <p style="font-size: 15px"><b style="font-size: 18px">VideoOrion: Tokenizing Object Dynamics in Videos</b><br>
        Yicheng Feng$^*$, Yijiang Li$^*$, Wanpeng Zhang, <b>Sipeng Zheng</b>, Zongqing Lu<br>
        arxiv 2024<br>
        [<a target="_blank" href="https://arxiv.org/abs/2411.16156">pdf</a>] 
        </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/motion_24.png" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
        <p style="font-size: 15px"><b style="font-size: 18px">Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models</b><br>
        Ye Wang$^*$, <b>Sipeng Zheng</b>$^*$, Bin Cao, Qianshan Wei, Qin Jin, Zongqing Lu<br>
        arxiv 2024<br>
          [<a target="_blank" href="https://arxiv.org/abs/2410.03311">pdf</a>] 
        [<a target="_blank" href="[https://arxiv.org/abs/2410.03311](http://www.motionbase3d.com">page</a>] 
        </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/2dbpe_24.png" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
        <p style="font-size: 15px"><b style="font-size: 18px">From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities</b><br>
        Wanpeng Zhang, Zilong Xie, Yicheng Feng, Yijiang Li, Xingrun Xing, <b>Sipeng Zheng</b>, Zongqing Lu<br>
        ICLR 2025<br>
        [<a target="_blank" href="https://arxiv.org/abs/2410.02155">pdf</a>] 
        </p>
  </td>
</tr>


<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/quargpt_24.png" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
        <p style="font-size: 15px"><b style="font-size: 18px">QuadrupedGPT: Towards a Versatile Quadruped Agent in Open-ended Worlds</b><br>
        Yuting Mei$^*$, Ye Wang$^*$, <b>Sipeng Zheng</b>, Qin Jin<br>
        arxiv 2024<br>
        [<a target="_blank" href="https://arxiv.org/pdf/2406.16578">pdf</a>] 
        [<a target="_blank" href="https://quadruped-hub.github.io/Quadruped-GPT/">page</a>] 
        </p>
  </td>
</tr>


<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/egonce++_24.png" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
        <p style="font-size: 15px"><b style="font-size: 18px">EgoNCE++: Do Egocentric Video-Language Models Really Understand Hand-Object Interactions?</b><br>
        Boshen Xu, Ziheng Wang, Yang Du, Zhinan Song, <b>Sipeng Zheng</b>, Qin Jin<br>
        ICLR 2025<br>
        [<a target="_blank" href="https://arxiv.org/html/2405.17719v1">pdf</a>] 
        [<a target="_blank" href="https://github.com/xuboshen/egoncepp">code</a>] 
        </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/spaformer_24.png" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
        <p style="font-size: 15px"><b style="font-size: 18px">SPAFormer: Sequential 3D Part Assembly with Transformers</b><br>
        Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
        3DV 2025<br>
        [<a target="_blank" href="https://arxiv.org/abs/2403.05874">pdf</a>] 
        [<a target="_blank" href="[https://github.com/xuboshen/SPAFormer](https://github.com/xuboshen/SPAFormer)">code</a>] 
        </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/unicode_24.png" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
        <p style="font-size: 15px"><b style="font-size: 18px">UniCode: Learning a Unified Codebook for Multimodal Large Language Models</b><br>
        <b>Sipeng Zheng</b>, Bohan Zhou, Yicheng Feng, Ye Wang, Zongqing Lu<br>
        ECCV 2024<br>
        [<a target="_blank" href="https://arxiv.org/abs/2310.13255">pdf</a>] 
        </p>
  </td>
</tr>



<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/steve_eye_23.png" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
        <p style="font-size: 15px"><b style="font-size: 18px">Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</b><br>
        <b>Sipeng Zheng</b>, Jiazheng Liu, Yicheng Feng, Zongqing Lu<br>
        ICLR 2024<br>
        [<a target="_blank" href="https://arxiv.org/abs/2310.13255">pdf</a>] 
        [<a target="_blank" href="https://github.com/BAAI-Agents/Steve-Eye">code</a>] 
        [<a target="_blank" href="https://sites.google.com/view/steve-eye">page</a>]
        </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/llama_rider_23.png" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
      <p style="font-size: 15px"><b style="font-size: 18px">LLaMA Rider: Spurring Large Language Models to Explore the Open World</b><br>
        Yicheng Feng, Yuxuan Wang, Jiazheng Liu, <b>Sipeng Zheng</b>, Zongqing Lu<br>
        NAACL 2024
        <br>
          [<a target="_blank" href="https://arxiv.org/abs/2310.08922">pdf</a>] 
          [<a target="_blank" href="https://github.com/PKU-RL/LLaMA-Rider">code</a>] 
      </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/mm23_pov.png" width="500px" height="100px"/></td>
  <td style="border: none;"> 
      <p style="font-size: 15px"><b style="font-size: 18px">POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World</b><br>
        Boshen Xu, <b>Sipeng Zheng</b>, Qin Jin<br>
        ACM MM, 2023
      <br>
          [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3581783.3612484">pdf</a>] 
          [<a target="_blank" href="https://github.com/xuboshen/pov_acmmm2023">code</a>]
          [<a target="_blank" href="https://xuboshen.github.io/POV/">page</a>] 
      </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/nofrill_23.png" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
      <p style="font-size: 15px"><b style="font-size: 18px">No-frills Temporal Video Grounding: Multi-Scale Neighboring Attention and Zoom-in Boundary Detection</b><br>
        Qi Zhang, <b>Sipeng Zheng</b>, Qin Jin<br>
        arxiv 2023
      <br>
          [<a target="_blank" href="https://arxiv.org/abs/2307.10567">pdf</a>] 
          [<a target="_blank" href="https://github.com/QiQAng/AwareNet">code</a>]
      </p>
  </td>
</tr>


<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/cvpr23_open.jpg" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
      <p style="font-size: 15px"><b style="font-size: 18px">Open-Category Human-Object Interaction Pre-Training via Language Modeling Framework</b><br>
        <b>Sipeng Zheng</b>, Boshen Xu, Qin Jin<br>
        CVPR, 2023
      <br>
          [<a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Open-Category_Human-Object_Interaction_Pre-Training_via_Language_Modeling_Framework_CVPR_2023_paper.pdf">pdf</a>] 
      </p>
  </td>
</tr>


<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/aaai23_audio.jpg" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
      <p style="font-size: 15px"><b style="font-size: 18px">Accommodating audio modality in CLIP for multimodal processing</b><br>
        Ludan Ruan, Anwen Hu, Yuqing Song, Lliang Zhang, <b>Sipeng Zheng</b>, Qin Jin<br>
        AAAI, 2023
      <br>
          [<a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/26153/25925">pdf</a>] 
      </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/cvpr22_nlq.jpg" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
      <p style="font-size: 15px"><b style="font-size: 18px">Anchor-Based Detection for Natural Language Localization in Ego-Centric Videos</b><br>
        <b>Sipeng Zheng</b>, Bei Liu, Jianlong Fu, Wen-Huang Cheng<br>
        IEEC, 2023
      <br>
          [<a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10043460">pdf</a>] 
          [<a target="_blank" href="https://github.com/QiQAng/AwareNet">code</a>]
      </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/eccv22_fewshot.jpg" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
      <p style="font-size: 15px"><b style="font-size: 18px">Few-shot Action Recognition with Hierarchical Matching and Contrastive Learning</b><br>
        <b>Sipeng Zheng</b>, Shizhe Chen, Qin Jin<br>
        ECCV, 2022
      <br>
          [<a target="_blank" href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640293.pdf">pdf</a>] 
          [<a target="_blank" href="https://github.com/zhengsipeng/HCL-FSAR">code</a>]
      </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/cvpr22_vrdformer.jpg" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
      <p style="font-size: 15px"><b style="font-size: 18px">VRDFormer: End-to-end video visual relation detection with transformer</b><br>
        <b>Sipeng Zheng</b>, Shizhe Chen, Qin Jin<br>
        CVPR Oral, 2022
      <br>
          [<a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_VRDFormer_End-to-End_Video_Visual_Relation_Detection_With_Transformers_CVPR_2022_paper.pdf">pdf</a>] 
          [<a target="_blank" href="https://github.com/zhengsipeng/VRDFormer_VRD">code</a>]
      </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/cvpr22_nlq.jpg" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
      <p style="font-size: 15px"><b style="font-size: 18px">Exploring anchor-based detection for ego4d natural language query</b><br>
        <b>Sipeng Zheng</b>, Qi Zhang, Bei Liu, Qin Jin, Jianlong Fu<br>
        CVPR Workshop, 2022
      <br>
          [<a target="_blank" href="https://arxiv.org/abs/2208.05375">pdf</a>] 
          [<a target="_blank" href="https://github.com/QiQAng/AwareNet">code</a>]
      </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/icme20_skeleton.jpg" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
      <p style="font-size: 15px"><b style="font-size: 18px">Skeleton-based interactive graph network for human object interaction detection</b><br>
        <b>Sipeng Zheng</b>, Shizhe Chen, Qin Jin<br>
        ICME, 2020
      <br>
          [<a target="_blank" href="https://ieeexplore.ieee.org/document/9102755">pdf</a>] 
          [<a target="_blank" href="https://github.com/zhengsipeng/SIGN">code</a>]
      </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/mm2019.jpg" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
      <p style="font-size: 15px"><b style="font-size: 18px">Visual relation detection with multi-level attention</b><br>
        <b>Sipeng Zheng</b>, Shizhe Chen, Qin Jin<br>
        ACM MM, 2019
      <br>
          [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3343031.3350962">pdf</a>] 
      </p>
  </td>
</tr>

<tr style="border: none;">
  <td style="border: none;"> <img src="./images/pubs/mm2019_gc.jpg" style="height: 100px; width: 500px;"/></td>
  <td style="border: none;"> 
      <p style="font-size: 15px"><b style="font-size: 18px">Relation understanding in videos</b><br>
        <b>Sipeng Zheng</b>, Xiangyu Chen, Shizhe Chen, Qin Jin<br>
        ACM MM Grand Challenge, 2019 
      <br>
          [<a target="_blank" href="https://dl.acm.org/doi/10.1145/3343031.3356080">pdf</a>] 
      </p>
  </td>
</tr>

</table>




  
## Awards
* National Scholarship for Ph.D Students.
* 2022 Ranked 3th in CVPR 2022 Ego4D Natural Language Query Challenge.
* 2021 Ranked 3th in NIST TRECVID 2021 Ad-hoc Video Search (AVS) Challenge. (20+ teams)
* 2021 Ranked 4th in CVPR 2021 HOMAGE Scene-graph Generation Challenge.
* 2020 Ranked 2th in ACM MM 2020 Video Relationship Understanding Grand Challenge.
* 2019 Ranked 2nd in ACM MM 2019 Video Relationship Understanding Grand Challenge.
* Best Method Prize in ACM MM 2019 Grand Challenge.
* 2019 First Class Scholarship for Ph.D Students from 2018 to 2021.
* 2015 First Prize in National University Mathematical Modeling Competition of Beijing Area.

## Services
* Conference Reviewer for CVPR, ICCV, ECCV, ACCV, NeurIPS, AAAI, ACM MM, ICME.
* Journal Reviewer for IJCV, TCSVT, TMM, JATS.
