
# üìù Publications 
<b>\* denotes equal contribution, ‚Ä† denotes project lead, ‚úâ denotes corresponding author</b>


<!-- BebingBeyond-->
## <img src='images/beingbeyond.ico' alt="beingbeyond" width="16" height="16"> BeingBeyond Series
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv</div><img src='images/arxiv_being_h05.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Being-H0.5**: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization](https://arxiv.org/pdf/2601.12993) \\
Hao Luo\*, Ye Wang\*, Wanpeng Zhang\*, **Sipeng Zheng\*<sup>‚Ä†</sup>**, Ziheng Xi, Chaoyi Xu, Haiweng Xu, Haoqi Yuan, Chi Zhang, Yiqing Wang, Yicheng Feng, Zongqing Lu<sup>‚úâ</sup>


[**Blog**](https://research.beingbeyond.com/being-h05) \|
<i class="fab fa-fw fa-github" aria-hidden="true"></i> 
[ **Code**](https://github.com/BeingBeyond/Being-H) \|
<img src='images/hg.ico' alt="huggingface" width="16" height="16"> 
[**Model**](https://huggingface.co/collections/BeingBeyond/being-h05)

- Robots do not just look different. They also act through different physical control languages: different kinematics, sensors, action conventions, and timing. **Being-H0.5** is our attempt to make one Vision-Language-Action model travel across those differences without turning into a brittle collection of per-robot hacks. The model is trained using over 35,000 hours of data, including 16,000 hours of human videos and 14,000 hours of robot manipualtion (30+ embodiments).

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv</div><img src='images/arxiv_being_h0.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Being-H0**: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597) \\
Hao Luo\*, Yicheng Feng\*, Wanpeng Zhang\*, **Sipeng Zheng\*<sup>‚Ä†</sup>**, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, Zongqing Lu<sup>‚úâ</sup>

[**Blog**](https://research.beingbeyond.com/being-h0) \|
<i class="fab fa-fw fa-github" aria-hidden="true"></i> 
[ **Code**](https://github.com/BeingBeyond/Being-H0) \|
<img src='images/hg.ico' alt="huggingface" width="16" height="16"> 
[**Model**](https://huggingface.co/BeingBeyond/Being-H0)

- Being-H0 is the first VLA pretrained from large-scale human videos with hand motion. 

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/iccv25_being-m05.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Being-M0.5**: A Real-Time Controllable Vision-Language-Motion Model](https://arxiv.org/pdf/2508.07863) \\
Bin Cao\*, **Sipeng Zheng\***, Ye Wang, Lujie Xia, Qianshan Wei, Qin Jin, Jing Liu, Zongqing Lu<sup>‚úâ</sup>

ICCV25

[**Blog**](https://research.beingbeyond.com/being-m05) \|
[**Page**](https://beingbeyond.github.io/Being-M0/)

- Being-M is the first large motion generation model scaling to million-level motion sequences.

[Being-M0: Scaling Large Motion Models with Million-Level Human Motions](https://arxiv.org/abs/2410.03311) (ICML 2025) 

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/iccv25_unified.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Being-VL0.5**: Unified Multimodal Understanding via Byte-Pair Visual Encoding](https://arxiv.org/abs/2506.23639) \\
Wanpeng Zhang, Yicheng Feng, Hao Luo, Yijiang Li, Zihao Yue, **Sipeng Zheng**, Zongqing Lu<sup>‚úâ</sup>

ICCV25 (<span style="color:red">Highlight</span>)

[**Blog**](https://research.beingbeyond.com/being-vl05)  \|
<i class="fab fa-fw fa-github" aria-hidden="true"></i> 
[**Code**](https://github.com/BeingBeyond/Being-VL-0.5) \|
[**Page**](https://github.com/BeingBeyond/Being-VL-0)

- Being-VL is the first large multimodal model based on compressed discrete visual representation using 2D-BPE.

[Being-VL-0: From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities](https://arxiv.org/abs/2410.02155) (ICLR 2025)

</div>
</div>




<!-- BEFORE BebingBeyond-->
## üéô Before BeingBeyond


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024</div><img src='images/iclr23_steve.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds](https://arxiv.org/abs/2310.13255), **Sipeng Zheng**, Jiazheng Liu, Yicheng Feng, Zongqing Lu<sup>‚úâ</sup> 

ICLR24 (<span style="color:red">Spotlight 5.02%</span>)

[**Project**](https://sites.google.com/view/steve-eye)  \|
<i class="fab fa-fw fa-github" aria-hidden="true"></i> [**Code**](https://github.com/BAAI-Agents/Steve-Eye)

</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2022</div><img src='images/eccv22_fewshot.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Few-shot Action Recognition with Hierarchical Matching and Contrastive Learning](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640293.pdf), 
**Sipeng Zheng**, Shizhe Chen, Qin Jin<sup>‚úâ</sup>

ECCV22

<i class="fab fa-fw fa-github" aria-hidden="true"></i> [**Code**](https://github.com/zhengsipeng/HCL-FSAR)
</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='images/cvpr22_vrdformer.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[VRDFormer: End-to-end video visual relation detection with transformer](https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_VRDFormer_End-to-End_Video_Visual_Relation_Detection_With_Transformers_CVPR_2022_paper.pdf), **Sipeng Zheng**, Shizhe Chen, Qin Jin<sup>‚úâ</sup>

CVPR22 (<span style="color:red">Oral 4.14%</span>) 

<i class="fab fa-fw fa-github" aria-hidden="true"></i> [**Code**](https://github.com/zhengsipeng/VRDFormer_VRD)
</div>
</div>



## üìö Paper List
- ``Arxiv 2026`` [Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization](https://arxiv.org/pdf/2601.12993), 
Hao Luo\*, Ye Wang\*, Wanpeng Zhang\*, **Sipeng Zheng\*<sup>‚Ä†</sup>**, Ziheng Xi, Chaoyi Xu, Haiweng Xu, Haoqi Yuan, Chi Zhang, Yiqing Wang, Yicheng Feng, Zongqing Lu<sup>‚úâ</sup>,
[**Blog**](https://research.beingbeyond.com/being-h05) | 
<i class="fab fa-fw fa-github" aria-hidden="true"></i> [**Code**](https://github.com/BeingBeyond/Being-H) | <img src='images/hg.ico' alt="huggingface" width="16" height="16"> 
[**Model**](https://huggingface.co/collections/BeingBeyond/being-h05).

- ``Arxiv 2025`` [Predictive Embedding as Latent Action: Towards VLA Pretraining in the Wild](https://openreview.net/pdf?id=iGwN4eoN6k), 
Hao Luo, Ye Wang, Wanpeng Zhang, Haoqi Yuan, Yicheng Feng, Haiweng Xu, **Sipeng Zheng**, Zongqing Lu<sup>‚úâ</sup>,

- ``Arxiv 2025`` [DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models](https://arxiv.org/pdf/2512.01715), 
Wanpeng Zhang, Ye Wang, Hao Luo, Haoqi Yuan, Yicheng Feng, **Sipeng Zheng**, Qin Jin, Zongqing Lu<sup>‚úâ</sup>,
[**Blog**](https://research.beingbeyond.com/dig-flow) | <i class="fab fa-fw fa-github" aria-hidden="true"></i> [**Code**](https://github.com/BeingBeyond/DiG-Flow).

- ``Arxiv 2025`` [Robust Motion Generation using Part-level Reliable Data from Videos](https://arxiv.org/pdf/2512.12703), 
Boyuan Li, **Sipeng Zheng**, Bin Cao, Ruihua Song, Zongqing Lu.

- ``Arxiv 2025`` [OpenT2M: No-frill Motion Generation with Open-source, Large-scale, High-quality Data](https://openreview.net/pdf?id=YcJnHKVB9v), 
Bin Cao, **Sipeng Zheng**, Hao Luo, Boyuan Li, Jing Liu, Zongqing Lu.

- ``Arxiv 2025`` [Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos](https://arxiv.org/abs/2512.13080), 
Yicheng Feng, Wanpeng Zhang, Ye Wang, Hao Luo, Haoqi Yuan, **Sipeng Zheng**, Zongqing Lu<sup>‚úâ</sup>,
[**Page**](https://beingbeyond.github.io/VIPA-VLA) | <i class="fab fa-fw fa-github" aria-hidden="true"></i> [**Project**](https://github.com/BeingBeyond/VIPA-VLA).

- ``Arxiv 2025`` [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597), 
Hao Luo\*, Yicheng Feng\*, Wanpeng Zhang\*, **Sipeng Zheng\*<sup>‚Ä†</sup>**, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, Zongqing Lu<sup>‚úâ</sup>, 
[**Blog**](https://research.beingbeyond.com/being-h0) | <i class="fab fa-fw fa-github" aria-hidden="true"></i> [**Code**](https://github.com/BeingBeyond/Being-H0) | <img src='images/hg.ico' alt="huggingface" width="16" height="16"> [**Model**](https://huggingface.co/BeingBeyond/Being-H0).

- ``Arxiv 2025`` [RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control](https://www.arxiv.org/abs/2506.12769), Junpeng Yue, Zepeng Wang, Yuxuan Wang, Weishuai Zeng, Jiangxing Wang, Xinrun Xu, Yu Zhang, **Sipeng Zheng**, Ziluo Ding, Zongqing Lu<sup>‚úâ</sup>, [**Page**](https://beingbeyond.github.io/RLPF/).

- ``NeurIPS 2025`` [OpenMMEgo: Enhancing Egocentric Understanding for LMMs with Open Weights and Data](https://openreview.net/forum?id=9OyMsbuzL5), Hao Luo, Zihao Yue, Wanpeng Zhang, Yicheng Feng, **Sipeng Zheng**, Deheng Ye, Zongqing Lu. 
Boshen Xu, Yuting Mei, Xinbi Liu, **Sipeng Zheng**, Jin Qin<sup>‚úâ</sup>.

- ``NeurIPS 2025`` [EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining](https://arxiv.org/abs/2503.15470), Boshen Xu, Yuting Mei, Xinbi Liu, **Sipeng Zheng**, Jin Qin<sup>‚úâ</sup>, <i class="fab fa-fw fa-github" aria-hidden="true"></i> [**Code**](https://github.com/BeingBeyond/Being-H0).

- ``EMNLP 2025`` [Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue Learning](https://arxiv.org/abs/2503.07002), Jiazheng Liu, **Sipeng Zheng**, B√∂rje F Karlsson, Zongqing Lu<sup>‚úâ</sup>.

- ``ICCV 2025`` [Unified Multimodal Understanding via Byte-Pair Visual Encoding](https://arxiv.org/abs/2506.23639), Wanpeng Zhang, Yicheng Feng, Hao Luo, Yijiang Li, Zihao Yue, **Sipeng Zheng**, Zongqing Lu<sup>‚úâ</sup>, [**Blog**](https://research.beingbeyond.com/being-vl05) 

- ``ICCV 2025`` [MotionCtrl: A Real-time Controllable Vision-Language-Motion Model](https://arxiv.org/abs/xxxx), Bin Cao\*, **Sipeng Zheng\***, Ye Wang, Lujie Xia, Qianshan Wei, Qin Jin, Jing Liu, Zongqing Lu<sup>‚úâ</sup>.

- ``ICCV 2025`` [VideoOrion: Tokenizing Object Dynamics in Videos](https://arxiv.org/abs/2411.16156), Yicheng Feng\*, Yijiang Li\*, Wanpeng Zhang, **Sipeng Zheng**, Zongqing Lu<sup>‚úâ</sup>.

- ``Arxiv 2025`` [QuadrupedGPT: Towards a Versatile Quadruped Agent in Open-ended Worlds](https://arxiv.org/pdf/2406.16578), Yuting Mei\*, Ye Wang\*, **Sipeng Zheng**, Qin Jin<sup>‚úâ</sup>, [**Page**](https://quadruped-hub.github.io/Quadruped-GPT/).

- ``ICML 2025`` [Scaling Large Motion Models with Million-Level Human Motions](https://arxiv.org/abs/2410.03311), Ye Wang\*, **Sipeng Zheng\***, Bin Cao, Qianshan Wei, Weishuai Zeng, Qin Jin, Zongqing Lu<sup>‚úâ</sup>, [**Page**](https://beingbeyond.github.io/Being-M0).

- ``ICLR 2025`` [EgoNCE++: Do Egocentric Video-Language Models Really Understand Hand-Object Interactions?](https://arxiv.org/html/2405.17719v1), Boshen Xu, Ziheng Wang, Yang Du, Zhinan Song, **Sipeng Zheng**, Qin Jin<sup>‚úâ</sup>, <i class="fab fa-fw fa-github" aria-hidden="true"></i> [**Code**](https://github.com/xuboshen/egoncepp).

- ``ICLR 2025`` [From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities](https://arxiv.org/abs/2410.02155), Wanpeng Zhang, Zilong Xie, Yicheng Feng, Yijiang Li, Xingrun Xing, **Sipeng Zheng**, Zongqing Lu<sup>‚úâ</sup>, [**Page**](https://github.com/BeingBeyond/Being-VL-0).

- ``3DV 2025`` [SPAFormer: Sequential 3D Part Assembly with Transformers](https://arxiv.org/abs/2403.05874), Boshen Xu, **Sipeng Zheng**, Qin Jin<sup>‚úâ</sup>, <i class="fab fa-fw fa-github" aria-hidden="true"></i> [**Code**](https://github.com/xuboshen/SPAFormer).

- ``ECCV 2024`` [UniCode: Learning a Unified Codebook for Multimodal Large Language Models](https://arxiv.org/abs/2403.09072), **Sipeng Zheng**, Bohan Zhou, Yicheng Feng, Ye Wang, Zongqing Lu<sup>‚úâ</sup>.
- ``ICLR 2024`` [Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds](https://arxiv.org/abs/2310.13255), **Sipeng Zheng**, Jiazheng Liu, Yicheng Feng, Zongqing Lu<sup>‚úâ</sup>.
- ``NAACL 2024`` [LLaMA Rider: Spurring Large Language Models to Explore the Open World](https://arxiv.org/abs/2310.08922), Yicheng Feng, Yuxuan Wang, Jiazheng Liu, **Sipeng Zheng**, Zongqing Lu<sup>‚úâ</sup>, [Project](https://github.com/PKU-RL/LLaMA-Rider).
- ``ACM-MM 2023`` [POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World](https://dl.acm.org/doi/10.1145/3581783.3612484), Boshen Xu, **Sipeng Zheng**, Qin Jin<sup>‚úâ</sup>, [Code](https://github.com/xuboshen/pov_acmmm2023) \| [Project](https://xuboshen.github.io/POV/).
- ``AAAI 2023`` [No-frills Temporal Video Grounding: Multi-Scale Neighboring Attention and Zoom-in Boundary Detection](https://arxiv.org/abs/2307.10567), Qi Zhang, **Sipeng Zheng**, Qin Jin<sup>‚úâ</sup>, [Code](https://github.com/QiQAng/AwareNet).
- ``CVPR 2023`` [Open-Category Human-Object Interaction Pre-Training via Language Modeling Framework](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Open-Category_Human-Object_Interaction_Pre-Training_via_Language_Modeling_Framework_CVPR_2023_paper.pdf), **Sipeng Zheng**, Boshen Xu, Qin Jin<sup>‚úâ</sup>.
- ``AAAI 2023`` [Accommodating audio modality in CLIP for multimodal processing](https://ojs.aaai.org/index.php/AAAI/article/view/26153/25925), Ludan Ruan, Anwen Hu, Yuqing Song, Lliang Zhang, **Sipeng Zheng**, Qin Jin<sup>‚úâ</sup>.
- ``IEEC 2023`` [Anchor-Based Detection for Natural Language Localization in Ego-Centric Videos](https://ieeexplore.ieee.org/abstract/document/10043460), **Sipeng Zheng**, Bei Liu, Jianlong Fu, Wen-Huang Cheng<sup>‚úâ</sup>, [Code](https://github.com/QiQAng/AwareNet).
- ``ECCV 2022`` [Few-shot Action Recognition with Hierarchical Matching and Contrastive Learning](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640293.pdf), **Sipeng Zheng**, Shizhen Chen, Qin Jin<sup>‚úâ</sup>.
- ``CVPR 2022`` [VRDFormer: End-to-end video visual relation detection with transformer](https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_VRDFormer_End-to-End_Video_Visual_Relation_Detection_With_Transformers_CVPR_2022_paper.pdf), **Sipeng Zheng**, Shizhe Chen, Qin Jin<sup>‚úâ</sup>, [Code](https://github.com/zhengsipeng/VRDFormer_VRD).
- ``CVPR 2022 workshop`` [Exploring anchor-based detection for ego4d natural language query](https://arxiv.org/abs/2208.05375), **Sipeng Zheng**, Qi Zhang, Bei Liu, Qin Jin<sup>‚úâ</sup>, Jianlong Fu, [Code](https://github.com/QiQAng/AwareNet).
- ``ICME 2020`` [Skeleton-based interactive graph network for human object interaction detection](https://ieeexplore.ieee.org/document/9102755), **Sipeng Zheng**, Shizhe Chen, Qin Jin<sup>‚úâ</sup>, [Code](https://github.com/zhengsipeng/SIGN).
- `ACM-MM 2019` [Visual relation detection with multi-level attention](https://dl.acm.org/doi/10.1145/3343031.3350962), **Sipeng Zheng**, Shizhe Chen, Qin Jin<sup>‚úâ</sup>.
- ``ACM-MM 2019`` [Relation understanding in videos](https://dl.acm.org/doi/10.1145/3343031.3356080), **Sipeng Zheng**, Xiangyu Chen, Shizhe Chen, Qin Jin<sup>‚úâ</sup>.