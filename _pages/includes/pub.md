
# üìù Publications 
<b>\* denotes equal contribution</b>

## <img src='images/favicon.ico' alt="beingbeyond" width="16" height="16"> BeingBeyond Series
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv</div><img src='images/arxiv_being_h0.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Being-H0**: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597) \\
Hao Luo\*, Yicheng Feng\*, Wanpeng Zhang\*, **Sipeng Zheng\***, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, Zongqing Lu

[**Project**](https://beingbeyond.github.io/Being-H0/) \|
<i class="fab fa-fw fa-github" aria-hidden="true"></i> [ **Code**](https://github.com/BeingBeyond/Being-H0) 

- Being-H0 is the first VLA pretrained from large-scale human videos with hand motion. 

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv</div><img src='images/arxiv_rlpf.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**RLPF**: Physical Feedback: Aligning Large Motion Models with Humanoid Control](https://arxiv.org/abs/2506.12769) \\
Junpeng Yue, Zepeng Wang, Yuxuan Wang, Weishuai Zeng, Jiangxing Wang, Xinrun Xu, Yu Zhang, **Sipeng Zheng**, Ziluo Ding, Zongqing Lu

[**Project**](https://beingbeyond.github.io/RLPF/)

- RLPF translates text-driven human motions into executable actions for humanoid robots.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/iccv25_being-m05.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Being-M0.5**: A Real-Time Controllable Vision-Language-Motion Model](https://arxiv.org/pdf/2508.07863) \\
Bin Cao\*, **Sipeng Zheng\***, Ye Wang, Lujie Xia, Qianshan Wei, Qin Jin, Jing Liu, Zongqing Lu

ICCV25

[**Project**](https://beingbeyond.github.io/Being-M0.5/)

- Being-M is the first large motion generation model scaling to million-level motion sequences.

[Being-M0: Scaling Large Motion Models with Million-Level Human Motions](https://arxiv.org/abs/2410.03311) (ICML 2025) 
\| [**page**](https://beingbeyond.github.io/Being-M0/)

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/iccv25_unified.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**Being-VL-0.5**: Unified Multimodal Understanding via Byte-Pair Visual Encoding](https://arxiv.org/abs/2506.23639) \\
Wanpeng Zhang, Yicheng Feng, Hao Luo, Yijiang Li, Zihao Yue, **Sipeng Zheng**, Zongqing Lu.  

ICCV25 (<span style="color:red">Highlight</span>)

[**Project**](https://beingbeyond.github.io/Being-VL-0.5)  \|
[**Code**](https://github.com/BeingBeyond/Being-VL-0.5) 

- Being-VL is the first large multimodal model based on compressed discrete visual representation using 2D-BPE.

[Being-VL-0: From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities](https://arxiv.org/abs/2410.02155) (ICLR 2025) 
\| [**page**](https://github.com/BeingBeyond/Being-VL-0)

</div>
</div>



## üéô Before BeingBeyond


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024</div><img src='images/iclr23_steve.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds](https://arxiv.org/abs/2310.13255), **Sipeng Zheng**, Jiazheng Liu, Yicheng Feng, Zongqing Lu. 

ICLR24 (<span style="color:red">Spotlight 5.02%</span>)

[**Project**](https://sites.google.com/view/steve-eye)  \|
[**Code**](https://github.com/BAAI-Agents/Steve-Eye)

</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2022</div><img src='images/eccv22_fewshot.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Few-shot Action Recognition with Hierarchical Matching and Contrastive Learning](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640293.pdf), 
**Sipeng Zheng**, Shizhe Chen, Qin Jin. 

ECCV22

[**Code**](https://github.com/zhengsipeng/HCL-FSAR)
</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022</div><img src='images/cvpr22_vrdformer.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[VRDFormer: End-to-end video visual relation detection with transformer](https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_VRDFormer_End-to-End_Video_Visual_Relation_Detection_With_Transformers_CVPR_2022_paper.pdf), **Sipeng Zheng**, Shizhe Chen, Qin Jin.

CVPR22 (<span style="color:red">Oral 4.14%</span>) 

[**Code**](https://github.com/zhengsipeng/VRDFormer_VRD)
</div>
</div>


## üìö Paper List
- ``Arxiv 2025`` [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597), 
Hao Luo\*, Yicheng Feng\*, Wanpeng Zhang\*, **Sipeng Zheng\***, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, Zongqing Lu
[Project](https://beingbeyond.github.io/Being-H0/).
- ``Arxiv 2025`` [RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control](https://www.arxiv.org/abs/2506.12769), Junpeng Yue, Zepeng Wang, Yuxuan Wang, Weishuai Zeng, Jiangxing Wang, Xinrun Xu, Yu Zhang, **Sipeng Zheng**, Ziluo Ding, Zongqing Lu [Project](https://beingbeyond.github.io/RLPF/).
- ``Arxiv 2025`` [EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining](https://arxiv.org/abs/2503.15470), Boshen Xu, Yuting Mei, Xinbi Liu, **Sipeng Zheng**, Jin Qin.
- ``Arxiv 2025`` [Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue Learning](https://arxiv.org/abs/2503.07002), Jiazheng Liu, **Sipeng Zheng**, B√∂rje F Karlsson, Zongqing Lu.
- ``ICCV 2025`` [Unified Multimodal Understanding via Byte-Pair Visual Encoding](https://arxiv.org/abs/2506.23639), Wanpeng Zhang, Yicheng Feng, Hao Luo, Yijiang Li, Zihao Yue, **Sipeng Zheng**, Zongqing Lu.
- ``ICCV 2025`` [MotionCtrl: A Real-time Controllable Vision-Language-Motion Model](https://arxiv.org/abs/xxxx), Bin Cao\*, **Sipeng Zheng\***, Ye Wang, Lujie Xia, Qianshan Wei, Qin Jin, Jing Liu, Zongqing Lu.
- ``ICCV 2025`` [VideoOrion: Tokenizing Object Dynamics in Videos](https://arxiv.org/abs/2411.16156), Yicheng Feng\*, Yijiang Li\*, Wanpeng Zhang, **Sipeng Zheng**, Zongqing Lu.
- ``Arxiv 2025`` [QuadrupedGPT: Towards a Versatile Quadruped Agent in Open-ended Worlds](https://arxiv.org/pdf/2406.16578), Yuting Mei\*, Ye Wang\*, **Sipeng Zheng**, Qin Jin, [Project](https://quadruped-hub.github.io/Quadruped-GPT/).
- ``ICML 2025`` [Scaling Large Motion Models with Million-Level Human Motions](https://arxiv.org/abs/2410.03311), Ye Wang\*, **Sipeng Zheng\***, Bin Cao, Qianshan Wei, Weishuai Zeng, Qin Jin, Zongqing Lu, [Project](https://beingbeyond.github.io/Being-M0).
- ``ICLR 2025`` [EgoNCE++: Do Egocentric Video-Language Models Really Understand Hand-Object Interactions?](https://arxiv.org/html/2405.17719v1), Boshen Xu, Ziheng Wang, Yang Du, Zhinan Song, **Sipeng Zheng**, Qin Jin, [Code](https://github.com/xuboshen/egoncepp).
- ``ICLR 2025`` [From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities](https://arxiv.org/abs/2410.02155), Wanpeng Zhang, Zilong Xie, Yicheng Feng, Yijiang Li, Xingrun Xing, **Sipeng Zheng**, Zongqing Lu, [Project](https://github.com/BeingBeyond/Being-VL-0).
- ``3DV 2025`` [SPAFormer: Sequential 3D Part Assembly with Transformers](https://arxiv.org/abs/2403.05874), Boshen Xu, **Sipeng Zheng**, Qin Jin, [Code](https://github.com/xuboshen/SPAFormer).
- ``ECCV 2024`` [UniCode: Learning a Unified Codebook for Multimodal Large Language Models](https://arxiv.org/abs/2403.09072), **Sipeng Zheng**, Bohan Zhou, Yicheng Feng, Ye Wang, Zongqing Lu.
- ``ICLR 2024`` [Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds](https://arxiv.org/abs/2310.13255), **Sipeng Zheng**, Jiazheng Liu, Yicheng Feng, Zongqing Lu.
- ``NAACL 2024`` [LLaMA Rider: Spurring Large Language Models to Explore the Open World](https://arxiv.org/abs/2310.08922), Yicheng Feng, Yuxuan Wang, Jiazheng Liu, **Sipeng Zheng**, Zongqing Lu, [Project](https://github.com/PKU-RL/LLaMA-Rider).
- ``ACM-MM 2023`` [POV: Prompt-Oriented View-agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World](https://dl.acm.org/doi/10.1145/3581783.3612484), Boshen Xu, **Sipeng Zheng**, Qin Jin, [Code](https://github.com/xuboshen/pov_acmmm2023) \| [Project](https://xuboshen.github.io/POV/).
- ``AAAI 2023`` [No-frills Temporal Video Grounding: Multi-Scale Neighboring Attention and Zoom-in Boundary Detection](https://arxiv.org/abs/2307.10567), Qi Zhang, **Sipeng Zheng**, Qin Jin, [Code](https://github.com/QiQAng/AwareNet).
- ``CVPR 2023`` [Open-Category Human-Object Interaction Pre-Training via Language Modeling Framework](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Open-Category_Human-Object_Interaction_Pre-Training_via_Language_Modeling_Framework_CVPR_2023_paper.pdf), **Sipeng Zheng**, Boshen Xu, Qin Jin.
- ``AAAI 2023`` [Accommodating audio modality in CLIP for multimodal processing](https://ojs.aaai.org/index.php/AAAI/article/view/26153/25925), Ludan Ruan, Anwen Hu, Yuqing Song, Lliang Zhang, **Sipeng Zheng**, Qin Jin.
- ``IEEC 2023`` [Anchor-Based Detection for Natural Language Localization in Ego-Centric Videos](https://ieeexplore.ieee.org/abstract/document/10043460), **Sipeng Zheng**, Bei Liu, Jianlong Fu, Wen-Huang Cheng, [Code](https://github.com/QiQAng/AwareNet).
- ``ECCV 2022`` [Few-shot Action Recognition with Hierarchical Matching and Contrastive Learning](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640293.pdf), **Sipeng Zheng**, Shizhen Chen, Qin Jin.
- ``CVPR 2022`` [VRDFormer: End-to-end video visual relation detection with transformer](https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_VRDFormer_End-to-End_Video_Visual_Relation_Detection_With_Transformers_CVPR_2022_paper.pdf), **Sipeng Zheng**, Shizhe Chen, Qin Jin, [Code](https://github.com/zhengsipeng/VRDFormer_VRD).
- ``CVPR 2022 workshop`` [Exploring anchor-based detection for ego4d natural language query](https://arxiv.org/abs/2208.05375), **Sipeng Zheng**, Qi Zhang, Bei Liu, Qin Jin, Jianlong Fu, [Code](https://github.com/QiQAng/AwareNet).
- ``ICME 2020`` [Skeleton-based interactive graph network for human object interaction detection](https://ieeexplore.ieee.org/document/9102755), **Sipeng Zheng**, Shizhe Chen, Qin Jin, [Code](https://github.com/zhengsipeng/SIGN).
- `ACM-MM 2019` [Visual relation detection with multi-level attention](https://dl.acm.org/doi/10.1145/3343031.3350962), **Sipeng Zheng**, Shizhe Chen, Qin Jin.
- ``ACM-MM 2019`` [Relation understanding in videos](https://dl.acm.org/doi/10.1145/3343031.3356080), **Sipeng Zheng**, Xiangyu Chen, Shizhe Chen, Qin Jin.