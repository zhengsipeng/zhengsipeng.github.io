---
title: "Exploring anchor-based detection for ego4d natural language query"
collection: publications
permalink: /publication/2022-6-paper-EGONLQ
excerpt: 'This paper proposes an egocentric framework for natural language query.'
date: 2022-6-01
venue: 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop 2022'
paperurl: 'https://arxiv.org/abs/2208.05375'
citation: 'S Zheng, Q Zhang, B Liu, Q Jin, J Fu. "Exploring anchor-based detection for ego4d natural language query." <i>2022 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop</i>. '
---
In this paper we provide the technique report of Ego4D natural language query challenge in CVPR 2022. Natural language query task is challenging due to the requirement of comprehensive understanding of video contents. Most previous works address this task based on third-person view datasets while few research interest has been placed in the ego-centric view by far. Great progress has been made though, we notice that previous works can not adapt well to ego-centric view datasets e.g., Ego4D mainly because of two reasons: 1) most queries in Ego4D have a excessively small temporal duration (e.g., less than 5 seconds); 2) queries in Ego4D are faced with much more complex video understanding of long-term temporal orders. Considering these, we propose our solution of this challenge to solve the above issues.



<figure style="text-align: center;">
<img src="../images/4-cvpr22_nlq.jpg" width="100%" height="100%" alt="替代文本">
<figcaption style="text-align: center; font-size: 18px">Figure 1:  Overview of our framework and an example of Ego4D query.</figcaption>
</figure>