---
title: "Anchor-Based Detection for Natural Language Localization in Ego-Centric Videos"
collection: publications
permalink: /publication/2023-1-paper-NLL
excerpt: 'This paper proposes a anchor-based detection method for ego-centric natural language localization.'
date: 2023-01-01
venue: '2023 IEEE International Conference on Consumer Electronics (ICCE)'
paperurl: 'https://ieeexplore.ieee.org/abstract/document/10043460/'
citation: 'B Liu, S Zheng, J Fu, WH Cheng. "Anchor-Based Detection for Natural Language Localization in Ego-Centric Videos." <i>2023 IEEE International Conference on Consumer Electronics (ICCE)</i>. 01-04.'
---
The Natural Language Localization (NLL) task aims to localize a sentence in a video with starting and ending timestamps. It requires a comprehensive understanding of both language and videos. We have seen a lot of work conducted for third-person view videos, while the task on ego-centric videos is still under-explored, which is critical for the understanding of increasing ego-centric videos and further facilitating embodied AI tasks. Directly adapting existing methods of NLL to ego-centric video datasets is challenging due to two reasons. Firstly, there is a temporal duration gap between different datasets. Secondly, queries in ego-centric videos usually require a better understanding of more complex and long-term temporal orders. For the above reason, we propose an anchor-based detection model for NLL in ego-centric videos.


<figure style="text-align: center;">
<img src="../images/4-cvpr22_nlq.jpg" width="100%" height="100%" alt="替代文本">
<figcaption style="text-align: center; font-size: 18px">Figure 1: Overall framework of our pipeline.</figcaption>
</figure>